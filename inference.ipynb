{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b]4;0;#000000\u001b\\\u001b]4;1;#FFFFFF\u001b\\\u001b]4;2;#CCCCCC\u001b\\\u001b]4;3;#8f8f8f\u001b\\\u001b]4;4;#FFFFFF\u001b\\\u001b]4;5;#111111\u001b\\\u001b]4;6;#CCCCCC\u001b\\\u001b]4;7;#FFFFFF\u001b\\\u001b]4;8;#404040\u001b\\\u001b]4;9;#CCCCCC\u001b\\\u001b]4;10;#FFFFFF\u001b\\\u001b]4;11;#909090\u001b\\\u001b]4;12;#CCCCCC\u001b\\\u001b]4;13;#808080\u001b\\\u001b]4;14;#CCCCCC\u001b\\\u001b]4;15;#FFFFFF\u001b\\\u001b]10;#FFFFFF\u001b\\\u001b]11;[100]#000000\u001b\\\u001b]12;#FFFFFF\u001b\\\u001b]13;#FFFFFF\u001b\\\u001b]17;#FFFFFF\u001b\\\u001b]19;#000000\u001b\\\u001b]4;232;#FFFFFF\u001b\\\u001b]4;256;#FFFFFF\u001b\\\u001b]708;[100]#000000\u001b\\\n",
      "\u001b]4;0;#000000\u001b\\\u001b]4;1;#FFFFFF\u001b\\\u001b]4;2;#CCCCCC\u001b\\\u001b]4;3;#8f8f8f\u001b\\\u001b]4;4;#FFFFFF\u001b\\\u001b]4;5;#111111\u001b\\\u001b]4;6;#CCCCCC\u001b\\\u001b]4;7;#FFFFFF\u001b\\\u001b]4;8;#404040\u001b\\\u001b]4;9;#CCCCCC\u001b\\\u001b]4;10;#FFFFFF\u001b\\\u001b]4;11;#909090\u001b\\\u001b]4;12;#CCCCCC\u001b\\\u001b]4;13;#808080\u001b\\\u001b]4;14;#CCCCCC\u001b\\\u001b]4;15;#FFFFFF\u001b\\\u001b]10;#FFFFFF\u001b\\\u001b]11;[100]#000000\u001b\\\u001b]12;#FFFFFF\u001b\\\u001b]13;#FFFFFF\u001b\\\u001b]17;#FFFFFF\u001b\\\u001b]19;#000000\u001b\\\u001b]4;232;#FFFFFF\u001b\\\u001b]4;256;#FFFFFF\u001b\\\u001b]708;[100]#000000\u001b\\\n",
      "\u001b]4;0;#000000\u001b\\\u001b]4;1;#FFFFFF\u001b\\\u001b]4;2;#CCCCCC\u001b\\\u001b]4;3;#8f8f8f\u001b\\\u001b]4;4;#FFFFFF\u001b\\\u001b]4;5;#111111\u001b\\\u001b]4;6;#CCCCCC\u001b\\\u001b]4;7;#FFFFFF\u001b\\\u001b]4;8;#404040\u001b\\\u001b]4;9;#CCCCCC\u001b\\\u001b]4;10;#FFFFFF\u001b\\\u001b]4;11;#909090\u001b\\\u001b]4;12;#CCCCCC\u001b\\\u001b]4;13;#808080\u001b\\\u001b]4;14;#CCCCCC\u001b\\\u001b]4;15;#FFFFFF\u001b\\\u001b]10;#FFFFFF\u001b\\\u001b]11;[100]#000000\u001b\\\u001b]12;#FFFFFF\u001b\\\u001b]13;#FFFFFF\u001b\\\u001b]17;#FFFFFF\u001b\\\u001b]19;#000000\u001b\\\u001b]4;232;#FFFFFF\u001b\\\u001b]4;256;#FFFFFF\u001b\\\u001b]708;[100]#000000\u001b\\\n"
     ]
    }
   ],
   "source": [
    "! pip install gradio --quiet\n",
    "! pip install opencv-python --quiet\n",
    "! pip install pillow --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skywalker/code/University/COMP702/Comp702_Project/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import gradio as gr\n",
    "\n",
    "\n",
    "def preprocess(img):\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    img = cv2.normalize(img, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX)\n",
    "    img = cv2.GaussianBlur(img, (5, 5), 0)\n",
    "    img = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)\n",
    "\n",
    "    kernel = np.ones((3,3),np.uint8)\n",
    "    img = cv2.morphologyEx(img, cv2.MORPH_OPEN, kernel)\n",
    "    return img\n",
    "    \n",
    "def contour_segmentation(img):\n",
    "    # img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    _, thresh = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "    # find the contours\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    # find the largest contour\n",
    "    max_area = 0\n",
    "    max_contour = None\n",
    "    for contour in contours:\n",
    "        area = cv2.contourArea(contour)\n",
    "        if area > max_area:\n",
    "            max_area = area\n",
    "            max_contour = contour\n",
    "    # find the bounding rectangle\n",
    "    x, y, w, h = cv2.boundingRect(max_contour)\n",
    "    # crop the image  into rect\n",
    "    img = img[y:y+h, x:x+w]\n",
    "    # img = img[y:y+(h if h < w else w), x:x+(h if h < w else w)]\n",
    "    return img\n",
    "\n",
    "def orientation(img):\n",
    "    # return img if img.shape[0] < img.shape[1] else cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE)\n",
    "    return img \n",
    "\n",
    "def extract_features_orb(img, n_features=1000):\n",
    "    orb = cv2.ORB_create(n_features)\n",
    "    keypoints, descriptors = orb.detectAndCompute(img, None)\n",
    "    return keypoints, descriptors\n",
    "\n",
    "def extract_features_sift(img):\n",
    "    sift = cv2.SIFT_create()\n",
    "    keypoints, descriptors = sift.detectAndCompute(img, None)\n",
    "    return keypoints, descriptors\n",
    "\n",
    "def extract_features_harris(img):\n",
    "    # remove alpha channel\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGRA2BGR)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    gray = np.float32(gray)\n",
    "    dst = cv2.cornerHarris(gray, 2, 3, 0.04)\n",
    "    dst = cv2.dilate(dst, None)\n",
    "    ret, dst = cv2.threshold(dst, 0.01 * dst.max(), 255, 0)\n",
    "    dst = np.uint8(dst)\n",
    "    ret, labels, stats, centroids = cv2.connectedComponentsWithStats(dst)\n",
    "    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 0.001)\n",
    "    corners = cv2.cornerSubPix(gray, np.float32(centroids), (5, 5), (-1, -1), criteria)\n",
    "    return corners\n",
    "\n",
    "def bag_of_words(descriptors, kmeans):\n",
    "    histogram = np.zeros(kmeans.n_clusters)\n",
    "    if descriptors is not None:\n",
    "        clusters = kmeans.predict(descriptors)\n",
    "        for cluster in clusters:\n",
    "            histogram[cluster] += 1\n",
    "    return histogram\n",
    "\n",
    "# additional more complex testing\n",
    "# some very specific examples that are difficult to classify\n",
    "# test_images, test_labels = load_images(data_test_dir)\n",
    "# preprocessed_test_images = [preprocess(img) for img in test_images]\n",
    "# aligned_test_images = [contour_segmentation(img) for img in preprocessed_test_images]\n",
    "# oriented_test_images = [orientation(img) for img in aligned_test_images]\n",
    "# # keypoints_test, descriptors_test = zip(*[extract_features_sift(img) for img in oriented_test_images])\n",
    "# # keypoints_test, descriptors_test = zip(*[extract_features_orb(img) for img in oriented_test_images])\n",
    "# descriptors_test = [extract_features_harris(img) for img in oriented_test_images]\n",
    "# bovw_test = np.array([bag_of_words(d, kmeans) for d in descriptors_test])\n",
    "# y_pred = rf.predict(bovw_test)\n",
    "# print('Accuracy:', accuracy_score(test_labels, y_pred))\n",
    "\n",
    "# y_pred = svm.predict(bovw_test)\n",
    "# print('Accuracy:', accuracy_score(test_labels, y_pred))\n",
    "\n",
    "# y_pred = knn.predict(bovw_test)\n",
    "# print('Accuracy:', accuracy_score(test_labels, y_pred))\n",
    "\n",
    "# # print oriented_test_images\n",
    "# display(preprocessed_test_images, oriented_test_images, test_labels, y_pred)\n",
    "\n",
    "# keypoints, descriptors = zip(*[extract_features_orb(img) for img in oriented])\n",
    "# keypoints, descriptors = zip(*[extract_features_sift(img) for img in oriented])\n",
    "# descriptors = [extract_features_harris(img) for img in oriented]\n",
    "# keypoints = [extract_features_hog(img) for img in oriented]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SIFT', 'ORB', 'HARRIS']\n",
      "{'SIFT_KMEANS': MiniBatchKMeans(n_clusters=1000), 'SIFT_SVC': SVC(C=1, kernel='linear', random_state=42), 'SIFT_RF': RandomForestClassifier(n_estimators=1000, random_state=42), 'SIFT_KNN': KNeighborsClassifier(n_neighbors=1), 'ORB_KMEANS': MiniBatchKMeans(n_clusters=1000), 'ORB_SVC': SVC(C=1, kernel='linear', random_state=42), 'ORB_RF': RandomForestClassifier(n_estimators=1000, random_state=42), 'ORB_KNN': KNeighborsClassifier(n_neighbors=1), 'HARRIS_KMEANS': KMeans(n_clusters=100), 'HARRIS_SVC': SVC(C=1, kernel='linear'), 'HARRIS_RF': RandomForestClassifier(random_state=42), 'HARRIS_KNN': KNeighborsClassifier(n_neighbors=1)}\n",
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://5b8baba5e9547cbf7a.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://5b8baba5e9547cbf7a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/skywalker/code/University/COMP702/Comp702_Project/.venv/lib/python3.12/site-packages/gradio/queueing.py\", line 532, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/skywalker/code/University/COMP702/Comp702_Project/.venv/lib/python3.12/site-packages/gradio/route_utils.py\", line 276, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/skywalker/code/University/COMP702/Comp702_Project/.venv/lib/python3.12/site-packages/gradio/blocks.py\", line 1923, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/skywalker/code/University/COMP702/Comp702_Project/.venv/lib/python3.12/site-packages/gradio/blocks.py\", line 1509, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/skywalker/code/University/COMP702/Comp702_Project/.venv/lib/python3.12/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/skywalker/code/University/COMP702/Comp702_Project/.venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 2177, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/home/skywalker/code/University/COMP702/Comp702_Project/.venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 859, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/skywalker/code/University/COMP702/Comp702_Project/.venv/lib/python3.12/site-packages/gradio/utils.py\", line 832, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_19907/3464677847.py\", line 22, in predict\n",
      "    img = preprocess(img)\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_19907/845888068.py\", line 14, in preprocess\n",
      "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "cv2.error: OpenCV(4.10.0) /io/opencv/modules/imgproc/src/color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/skywalker/code/University/COMP702/Comp702_Project/.venv/lib/python3.12/site-packages/gradio/queueing.py\", line 532, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/skywalker/code/University/COMP702/Comp702_Project/.venv/lib/python3.12/site-packages/gradio/route_utils.py\", line 276, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/skywalker/code/University/COMP702/Comp702_Project/.venv/lib/python3.12/site-packages/gradio/blocks.py\", line 1923, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/skywalker/code/University/COMP702/Comp702_Project/.venv/lib/python3.12/site-packages/gradio/blocks.py\", line 1509, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/skywalker/code/University/COMP702/Comp702_Project/.venv/lib/python3.12/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/skywalker/code/University/COMP702/Comp702_Project/.venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 2177, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/home/skywalker/code/University/COMP702/Comp702_Project/.venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 859, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/skywalker/code/University/COMP702/Comp702_Project/.venv/lib/python3.12/site-packages/gradio/utils.py\", line 832, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_19907/3464677847.py\", line 22, in predict\n",
      "    img = preprocess(img)\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_19907/845888068.py\", line 14, in preprocess\n",
      "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "cv2.error: OpenCV(4.10.0) /io/opencv/modules/imgproc/src/color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "model_dir = \"models\"\n",
    "feats = os.listdir(model_dir)\n",
    "classies = [\"svc\", \"rf\", \"knn\"]\n",
    "print(feats)\n",
    "\n",
    "def load_models():\n",
    "    models = {}\n",
    "    for model in feats:\n",
    "        models[f\"{model}_kmeans\".upper()] = joblib.load(os.path.join(model_dir, model, \"kmeans.pkl\"))\n",
    "        for clas in classies:\n",
    "            model_path = os.path.join(model_dir, model, f\"{clas}.pkl\")\n",
    "            models[f\"{model}_{clas}\".upper()] = joblib.load(model_path)\n",
    "    return models\n",
    "\n",
    "models = load_models()\n",
    "print(models)\n",
    "def predict(model_name, model_feat_extract, img):\n",
    "    img = preprocess(img)\n",
    "    img = contour_segmentation(img)\n",
    "    img = orientation(img)\n",
    "    if model_feat_extract == \"ORB\":\n",
    "        keypoints, descriptors = extract_features_orb(img)\n",
    "    elif model_feat_extract == \"SIFT\":\n",
    "        keypoints, descriptors = extract_features_sift(img)\n",
    "    elif model_feat_extract == \"HARRIS\":\n",
    "        descriptors = extract_features_harris(img)\n",
    "\n",
    "    kmeans = models[f\"{model_feat_extract}_KMEANS\"]\n",
    "    print(\"here\")\n",
    "    bovw = bag_of_words(descriptors, kmeans)\n",
    "    model = models[f\"{model_feat_extract}_{model_name}\"]\n",
    "    y_pred = model.predict([bovw])\n",
    "    return y_pred[0], img\n",
    "    \n",
    "\n",
    "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
    "    with gr.Row():\n",
    "        model = gr.Dropdown(label=\"Classifier\", choices=[\"SVC\", \"KNN\", \"RF\"], interactive=True, value=\"RF\")\n",
    "        model_feat_extract = gr.Dropdown(label=\"Feat Extract\", choices=[\"ORB\", \"SIFT\", \"HARRIS\"], interactive=True, value=\"SIFT\")\n",
    "        prediction = gr.Label(\"\")\n",
    "    with gr.Row():\n",
    "        img = gr.Image()\n",
    "        out_img = gr.Image()\n",
    "        \n",
    "\n",
    "    # greet_btn.click(predict, [model, model_feat_extract, img] , [prediction, out_img])\n",
    "    img.upload(predict, [model, model_feat_extract, img] , [prediction, out_img])\n",
    "    model_feat_extract.select(predict, [model, model_feat_extract, img] , [prediction, out_img])\n",
    "    model.select(predict, [model, model_feat_extract, img] , [prediction, out_img])\n",
    "\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
